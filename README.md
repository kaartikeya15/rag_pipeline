**RAG Pipeline with FastAPI & Mistral AI**

This project implements a Retrieval-Augmented Generation (RAG) system from scratch. It ingests PDF documents, extracts and chunks text, embeds those chunks, and enables querying via semantic + keyword search. Responses are generated through Mistral AI LLM, with citations to the supporting document passages.

â¸»

**Key Features**
	**PDF Ingestion & Chunking**
	â€¢	Upload one or more PDF files via FastAPI.
	â€¢	Text extracted with PyPDF2 and cleaned (whitespace normalization, empty-page skip).
	â€¢	Chunking uses a sliding window with overlap (CHUNK_SIZE and CHUNK_OVERLAP configurable).
	â€¢	Each chunk is tokenized and indexed with:
	â€¢	Semantic embeddings (for similarity search).
	â€¢	Term-frequency maps (for keyword matching).
	**Query Processing**
	â€¢	Lightweight intent detection (e.g., greetings like â€œhelloâ€ donâ€™t trigger retrieval).
	â€¢	Query normalization (lowercasing, whitespace trimming).
	â€¢	Prepared for more advanced query rewriting if extended.
	**Hybrid Semantic Search**
	â€¢	Cosine similarity on embeddings + TF-IDFâ€“style keyword scores combined.
	â€¢	Balances semantic meaning with keyword precision.
	â€¢	No external libraries or vector databases â€” all retrieval implemented in SQLite + NumPy.
	**Post-processing**
	â€¢	Top-k results merged and re-ranked.
	â€¢	Threshold check: if average similarity is below COSINE_THRESHOLD, system refuses with â€œInsufficient evidence to answer confidently.â€
	â€¢	Results contextualized and passed to the LLM with citations.
	**Answer Generation**
	â€¢	Mistral AI API (mistralai) used for completions.
	â€¢	System prompt enforces grounding: â€œAnswer only using the provided context. Cite sources.â€
	â€¢	Answers include inline citations: [doc:page:chunk].
	**Web UI**
	â€¢	Modern chat-style frontend with:
	â€¢	PDF upload
	â€¢	Reset button to clear KB
	â€¢	Smooth chat bubbles (user/right, assistant/left)
	â€¢	Auto-scroll and enter-to-send
	â€¢	Built with vanilla HTML/CSS/JS for simplicity.
	**Bonus Features**
	â€¢	No third-party vector DB (embeddings stored in SQLite).
	â€¢	Citations required.
	â€¢	Extendable with:
	â€¢	Answer shaping (e.g., structured tables).
	â€¢	Hallucination filters (check unsupported claims).
	â€¢	Query refusal for PII/legal/medical questions.

â¸»

**System Architecture**

flowchart TD
    A[ðŸ“„ PDF Upload] --> B[Text Extraction & Cleaning]
    B --> C[Chunking + Tokenization]
    C --> D[Embeddings via Mistral API]
    C --> E[Keyword Stats (TF)]
    D --> F[SQLite DB Storage]
    E --> F
    G[ðŸ” Query] --> H[Intent Detection]
    H --> I[Hybrid Search (Semantic + Keyword)]
    I --> J[Threshold Check]
    J -->|Low similarity| K[Refusal: "Insufficient evidence"]
    J -->|Sufficient| L[Context Assembly]
    L --> M[LLM (Mistral Chat)]
    M --> N[Answer + Citations]
    N --> O[ðŸ’¬ Web UI]


â¸»

**Tech Stack**
	â€¢	Backend: FastAPI
	â€¢	LLM: Mistral AI API
	â€¢	Text Extraction: PyPDF2
	â€¢	Database: SQLite (stores docs, chunks, embeddings, TF maps)
	â€¢	Vector Math: NumPy
	â€¢	Frontend: HTML/CSS/JavaScript

â¸»

**Project Structure**

rag_pipeline/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py          # FastAPI entry (ingest, query, reset)
â”‚   â”œâ”€â”€ pdf_ingest.py    # Extraction, cleaning, chunking, embeddings
â”‚   â”œâ”€â”€ retrieval.py     # Hybrid search logic
â”‚   â”œâ”€â”€ store.py         # SQLite persistence
â”‚   â”œâ”€â”€ config.py        # Settings (chunk size, thresholds, API keys)
â”‚   â””â”€â”€ database.py      # DB init and connection
â”œâ”€â”€ static/
â”‚   â””â”€â”€ index.html       # Chat UI
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md


â¸»

**Setup & Run**

1. Clone repo & create venv

git clone https://github.com/<your-username>/rag_pipeline.git
cd rag_pipeline
python3 -m venv venv
source venv/bin/activate

2. Install dependencies

pip install -r requirements.txt

3. Configure API Key

Create .env:

MISTRAL_API_KEY=your_api_key_here
CHUNK_SIZE=500
CHUNK_OVERLAP=50
TOP_K=3
COSINE_THRESHOLD=0.75

4. Run server

uvicorn app.main:app --reload

5. Access UI

Go to: http://127.0.0.1:8000/static/index.html

â¸»

ðŸ“Œ Example Usage
	â€¢	Upload:

POST /ingest (multipart/form-data)

Returns ingested docs.

	â€¢	Query:

POST /query
{ "query": "Summarize this document" }

Returns:

{
  "answer": "Revenue grew by 7% ... [doc:file.pdf:p44:366]",
  "sources": ["file.pdf"]
}


	â€¢	Reset KB:

POST /reset



â¸»

**Design Considerations**
	â€¢	Chunking with overlap: avoids semantic breaks mid-sentence; overlap preserves context continuity.
	â€¢	SQLite instead of vector DB: keeps dependencies light, matches bonus point requirement.
	â€¢	Hybrid retrieval: semantic search handles paraphrasing, keywords catch exact matches.
	â€¢	Threshold refusal: prevents hallucinations by enforcing evidence.
	â€¢	Citations: ensures transparency and trustworthiness.
	â€¢	Extensibility: UI and backend designed to plug in more advanced intent classification, structured answer shaping, and safety policies.

â¸»

**Author**

Kaartikeya Panjwani
Masterâ€™s in CS, NYU Courant Institute
Projects in ML, RAG, and full-stack development.
