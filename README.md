# ğŸ“š RAG Pipeline with FastAPI & Mistral AI

This project implements a **Retrieval-Augmented Generation (RAG)** system from scratch.  
It ingests PDF documents, extracts and chunks text, embeds those chunks, and enables querying via semantic + keyword search.  
Responses are generated through **Mistral AI LLM**, with citations to the supporting document passages.

---

## âœ¨ Key Features

### ğŸ“„ PDF Ingestion & Chunking
- Upload one or more PDF files via FastAPI.
- Text extracted with **PyPDF2** and cleaned (whitespace normalization, empty-page skip).
- Chunking uses a **sliding window with overlap** (`CHUNK_SIZE` and `CHUNK_OVERLAP` configurable).
- Each chunk is tokenized and indexed with:
  - Semantic embeddings (for similarity search).
  - Term-frequency maps (for keyword matching).

### ğŸ¤– Query Processing
- Lightweight **intent detection** (e.g., greetings like â€œhelloâ€ donâ€™t trigger retrieval).
- Query normalization (lowercasing, whitespace trimming).
- Prepared for more advanced query rewriting if extended.

### ğŸ” Hybrid Semantic Search
- **Cosine similarity** on embeddings + **TF-IDFâ€“style keyword scores** combined.
- Balances semantic meaning with keyword precision.
- No external libraries or vector databases â€” **all retrieval implemented in SQLite + NumPy**.

### ğŸ“ Post-processing
- Top-k results merged and re-ranked.
- **Threshold check**: if average similarity is below `COSINE_THRESHOLD`, system refuses with  
  *â€œInsufficient evidence to answer confidently.â€*
- Results contextualized and passed to the LLM with citations.

### ğŸ’¬ Answer Generation
- **Mistral AI API (`mistralai`)** used for completions.
- System prompt enforces grounding: *â€œAnswer only using the provided context. Cite sources.â€*
- Answers include inline citations: `[doc:page:chunk]`.

### ğŸ¨ Web UI
- Modern **chat-style frontend** with:
  - PDF upload
  - Reset button to clear KB
  - Smooth chat bubbles (user/right, assistant/left)
  - Auto-scroll and enter-to-send
- Built with **vanilla HTML/CSS/JS** for simplicity.

### ğŸ›¡ï¸ Bonus Features
- âœ… No third-party vector DB (embeddings stored in SQLite).
- âœ… Citations required.
- ğŸ”œ Extendable with:
  - Answer shaping (e.g., structured tables).
  - Hallucination filters (check unsupported claims).
  - Query refusal for PII/legal/medical questions.

---

## ğŸ–‡ï¸ System Architecture


flowchart TD
    A[ğŸ“„ PDF Upload] --> B[Text Extraction & Cleaning]
    B --> C[Chunking + Tokenization]
    C --> D[Embeddings via Mistral API]
    C --> E[Keyword Stats (TF)]
    D --> F[SQLite DB Storage]
    E --> F
    G[ğŸ” Query] --> H[Intent Detection]
    H --> I[Hybrid Search (Semantic + Keyword)]
    I --> J[Threshold Check]
    J -->|Low similarity| K[Refusal: "Insufficient evidence"]
    J -->|Sufficient| L[Context Assembly]
    L --> M[LLM (Mistral Chat)]
    M --> N[Answer + Citations]
    N --> O[ğŸ’¬ Web UI]
```

---

## ğŸ› ï¸ Tech Stack
- **Backend:** FastAPI  
- **LLM:** Mistral AI API  
- **Text Extraction:** PyPDF2  
- **Database:** SQLite (stores docs, chunks, embeddings, TF maps)  
- **Vector Math:** NumPy  
- **Frontend:** HTML/CSS/JavaScript  

---

## ğŸ“‚ Project Structure

```
rag_pipeline/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py          # FastAPI entry (ingest, query, reset)
â”‚   â”œâ”€â”€ pdf_ingest.py    # Extraction, cleaning, chunking, embeddings
â”‚   â”œâ”€â”€ retrieval.py     # Hybrid search logic
â”‚   â”œâ”€â”€ store.py         # SQLite persistence
â”‚   â”œâ”€â”€ config.py        # Settings (chunk size, thresholds, API keys)
â”‚   â””â”€â”€ database.py      # DB init and connection
â”œâ”€â”€ static/
â”‚   â””â”€â”€ index.html       # Chat UI
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## âš™ï¸ Setup & Run

### 1. Clone repo & create venv
```bash
git clone https://github.com/<your-username>/rag_pipeline.git
cd rag_pipeline
python3 -m venv venv
source venv/bin/activate
```

### 2. Install dependencies
```bash
pip install -r requirements.txt
```

### 3. Configure API Key
Create `.env`:
```
MISTRAL_API_KEY=your_api_key_here
CHUNK_SIZE=500
CHUNK_OVERLAP=50
TOP_K=3
COSINE_THRESHOLD=0.75
```

### 4. Run server
```bash
uvicorn app.main:app --reload
```

### 5. Access UI
Open: [http://127.0.0.1:8000/static/index.html](http://127.0.0.1:8000/static/index.html)

---

## ğŸ“Œ Example Usage

### Upload
```
POST /ingest (multipart/form-data)
```
Response:
```json
{ "document_id": 1, "name": "file.pdf" }
```

### Query
```
POST /query
{ "query": "Summarize this document" }
```
Response:
```json
{
  "answer": "Revenue grew by 7% ... [doc:file.pdf:p44:366]",
  "sources": ["file.pdf"]
}
```

### Reset KB
```
POST /reset
```

---

## ğŸ’¡ Design Considerations
- **Chunking with overlap**: avoids semantic breaks mid-sentence; overlap preserves context continuity.  
- **SQLite instead of vector DB**: keeps dependencies light, matches bonus point requirement.  
- **Hybrid retrieval**: semantic search handles paraphrasing, keywords catch exact matches.  
- **Threshold refusal**: prevents hallucinations by enforcing evidence.  
- **Citations**: ensures transparency and trustworthiness.  
- **Extensibility**: UI and backend designed to plug in more advanced intent classification, structured answer shaping, and safety policies.  

---

## ğŸ‘¤ Author
**Kaartikeya Panjwani**  
Masterâ€™s in CS, NYU Courant Institute  
Projects in ML, RAG, and full-stack development.  
